# 一、操作系统

## 1.进程和线程

1. 进程是资源分配的基本单位，运行一个可执行程序可能运行一至多个进程，进程就是运行起来的可执行的程序；
2. 线程是独立调度、程序执行的基本单位，一个进程只有唯一的主线程；
3. 进程拥有CPU资源、内存资源、文件资源和句柄等，线程拥有程序计数器、寄存器、状态字和栈；
4. 进程的切换设计进程CPU环境的保存以及新调度的进程的CPU环境设置，开销较大、速度慢；线程只需要保存和设置少许的寄存器内容。

## 2.线程比进程具有哪些优势？

1. 线程的划分尺度比进程小，因此多线程程序的并发性高；
2. 进程在执行过程中拥有独立的内存单元，而多个线程共享进程的内存与资源，极大地提高了程序的运行效率；
3. 线程切换的开销小、速度快，但不利于资源的管理和保护。

## 3.一个进程可以创建多少线程？

​		理论上，一个进程可用虚拟空间是2G，默认情况下，线程的栈的大小是1MB，所以理论上最多只能创建2048个线程。如果要创建多于2048的话，必须修改编译器的设置。

## 4.外中断和异常

- 外中断：是指由CPU执行指令以外的事件引起，如I/O完成终端等；
- 异常（内中断）：CPU执行指令的内部事件引起，如地址越界、算术溢出等。

## ？5.进程线程模型你知道多少？

## 6.进程调度算法

- 先来先服务算法
- 短作业优先
- 最短剩余时间优先
- 高响应比优先
- 时间片轮转
- 优先级调度
- 多级反馈队列

## 7.Linux下进程间通信方式？

- 管道
  - 有名管道FIFO
  - 匿名管道
- 共享内存：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的IPC方式，它是针对其他进程间通信方式运行效率低而专 门设计的。它往往与信号量，配合使用来实现进程间的同步和通信。
- 消息队列：消息队列是有消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
- 套接字
- 信号：用于通知接收进程某个事件已经发生，比如按下ctrl + C就是信号。
- 信号量

## 8.Linux下同步机制

1. 锁机制
   - 条件变量
   - 读写锁
   - 互斥锁
   - 自旋锁
2. 信号量
3. 信号
4. 屏障

## 9.什么是共享？

​		共享就是多个并发进程共用系统中的资源。

​		有两种共享方式：互斥共享和同时共享。 互斥共享的资源称为临界资源，例如打印机等，在同一时刻只允许一个进程访问，需要用同步机制来实现互斥访问。

## 10.动态分区分配算法有哪几种？

1. 首次适应算法：每次都从低地址开始查找，找到第一个能满足大小的空闲分区；

2. 最佳适应算法：优先使用更小的连续空闲区；

   缺点：每次都选最小的分区进行分配，会留下越来越多的很小且难以利用的内存块，这就是外部碎片；

3. 最坏适应算法：优先使用最大的连续空闲区；

4. 邻近适应算法：首次适应算法每次都从链头开始查找的。这可能会导致低地址部分出现很多小的空闲分区， 而每次分配查找时，都要经过这些分区，因此也增加了查找的开销。如果每次都从上次查找结束的位置 开始检索，就能解决上述问题。

## 11.内部碎片和外部碎片

- 内部碎片：分配给某些进程的内存区域上有些部分没用上，常见于固定分区分配。

  ​		内存总量相同，100M。

  - 分页式分配，固定分配，将100M分割成10块，每块10M，一个程序需要45M，那么需要分配5块，第五块只用了 5M，剩下的5M就是内部碎片； 
  - 分段式分配，按需分配，一个程序需要45M，就给分片45MB，剩下的55M供其它程序使用，不存在内部碎片。

- 外部碎片：内存中某些空闲区因为比较小，而难以利用，一般出现在内存动态分配方式中。

  - 分段式分配：内存总量相同，100M，比如内存分配依次5M，15M，50M，25M，程序运行一段时间 之后，5M，15M的程序运行完毕，释放内存，其他程序还在运行，再次分配一个10M的内存供其它程序使用，只能从头开始分片，这样，就会存在10M+5M的外部碎片

## ？12.如何消除碎片文件？

## 13.为什么分段式存储管理有外部碎片而无内部碎片？为什么固定分区分配有内部碎片而不会有外部碎片？

​		分段式分配是按需分配，而固定式分配是固定分配的方式

## 14.一个程序从开始运行到结束的完整过程

### （1）预编译（处理.c文件）

​		主要处理源代码文件中的以"#"开头的预编译指令。

1. 删除所有的#define，展开宏定义;
2. 处理所有的条件预编译指令;
3. 处理#include预编译指令，将文件内容替换到它的位置，这个过程是递归的；
4. 删除所有注释；
5. 保留所有的#pragma指令
6. 添加行号和文件表示，便于编译时编译器产生调试用的行号信息，和编译时产生编译错误或警告时能够显示行号。

### （2）编译（处理.i或.ii文件）

​		翻译成汇编文件，编译器进行词法分析、语法分析、语义分析、中间代码生成、目标代码生成和优化。

### （3）汇编（处理.s文件）

​		将汇编代码转变成机器可执行的指令（可重定位目标文件）。

​		Linux下生成.o文件，Windows下生成.obj文件

​		PS：**重定位就是将输入的多个可重定位目标文件合并为一个可执行目标文件。**

### （4）链接（处理.o文件）

​		将不同源文件生成的可重定位目标文件进行链接，从而形成一个可以执行的程序。

## 15.静态链接和动态链接？

## 16.进程同步的方法？

## 17.操作系统在对内存进行管理时需要做些什么？

- 操作系统需要负责对内存空间的分配与回收；
- 操作系统提供某种技术从逻辑上对内存空间扩充；
- 操作系统需要提供地址转换功能，负责程序的逻辑地址与物理地址的转换;
-  操作系统需要提供内存保护功能。保证各进程在各自存储空间内运行，互不干扰。

## 18.虚拟技术你了解吗？

​		虚拟技术即把一个物理实体转换为多个逻辑实体。

​		主要有两种虚拟技术：时分复用技术和空分复用技术。

- 时分复用技术：多个进程能在同一个处理器上并发执行，是因为每个进程轮流占用处理器，每次只执行一小个时间片并快速切换。
- 空分复用技术：虚拟内存用到了空分复用技术。**它将物理内存抽象为地址空间，每个进程都有各自的地址空间。**地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页置换到内存中。

## 19.虚拟内存的目的与实现？

​		虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。

​		为了更好的管理内存，操作系统将物理内存抽象成地址空间。**每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。**

​		这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中**。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。**

## 20.什么是内存？

​		内存是用于存放数据的硬件，程序执行前需要先放到内存中才能被CPU处理。

## 21.进程状态的切换？

![image-20210406205329710](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210406205329710.png)

- 就绪态：进程获得了除处理机之外的一切资源，一旦得到处理机便可执行；
- 运行态：进程得到所有资源后正常执行。单处理机环境下，每个时刻只能由一个进程处于运行态;
- 阻塞态：进程正在等待某一事件而暂停运行，此时即使得到处理机资源也不能运行。

## 22.内存交换和覆盖有什么区别？

​		交换是不同进程之间进行，而覆盖则用于同一进程。

## 23.内存的覆盖是什么？有什么特点？

​		由于程序运行时并非任何时候都要访问程序及数据的各个部分，因此可以把用户空间分成为一个固定区和若干个覆盖区。将经常活跃的部分放在固定区，其余部分按照调用关系分段，首先 将那些即将要访问的段放入覆盖区，其他段放在外存中**，在需要调用前，系统将其调入覆盖区，替换覆盖区中原有的段。**

## 24.内存的交换是什么？有什么特点？

​		交换(对换)技术的设计思想：**内存空间紧张时，系统将内存中某些进程暂时换出外存，把外存中某些已具备运行条件的进程换入内存**(进程在内存与磁盘间动态调度)

## 25.抖动现象？（颠簸现象）

​		刚刚换出的页面马上又要换入内存，刚刚换入的页面马上又要换出外存，这种频繁的页面调度行为称为 抖动，或颠簸。产生抖动的主要原因是进程频繁访问的页面数目高于可用的物理块数(分配给进程的物理 块不够)

## 26.交换空间与虚拟内存的关系？

- 交换空间：Linux中的交换空间在物理内存（RAM）被充满时被使用。如果系统需要更多的内存资源，而物理内存已经充满，内存中不活跃的页就会被移到交换空间去。

  交换空间的总大小应该相当于你的计算机内存的两倍和32 MB这两个值中较大的一个，但是它不能超过 2048 MB（2 GB）。

- 虚拟内存：虚拟内存使用的是硬盘的空间，Windows为了防止程序过大而使内存消耗殆尽，需要拿出一部分硬盘空间来充当内存使用。

## 27.内存交换中，被换出的进程保存在哪？

​		保存在磁盘中，也就是外存中。

​		具有对换功能的操作系统中，通常**把磁盘空间分为文件区和对换区两部分**。文件区主要用于存放文件，主要追求存储空间的利用率，因此对**文件区空间的管理采用离散分配方式**;对换区空间只占磁盘空间的小部分，被换出的进程数据就存放在对换区。由于对换的速度直接影响到系统的整体速度，因此对换区空间的管理主要追求换入换出速度，因此通常**对换区采用连续分配方式**

## 28.在发生内存交换时，有哪些进程是被优先考虑的？

​		可优先换出阻塞进程；换出优先级低的进程，为了防止优先级低的进程在被调入内存后很快又被换出， 有的系统还会考虑进程在内存的驻留时间。

​		PCB会常驻内存，不会被换出外存

## 29.内存交换需要注意的关键点？

1. 交换需要备份存储，通常是快速磁盘，它必须足够大，**且提供对这些内存映像的直接访问**。
2. 为有效使用CPU，需要每个进程的执行时间比交换时间长，而影响交换时间的主要是转移时间， 转移时间与所交换的空间内存成正比。
3. 交换空间通常作为磁盘的一整块，且独立于文件系统，因此使用就可能很快。
4. 交换通常在有许多进程运行且内存空间吃紧时开始启动，而系统负荷降低就暂停。

## 30.什么时候会进行内存的交换？

​		内存交换通常在许多进程运行且内存吃紧时进行，而系统负荷降低就暂停。

## 31.介绍一下几种经典的锁

### （1）读写锁

- 允许多个线程同时读共享数据
- 对写操作是互斥的
- 写者优先与读者

### （2）互斥锁（mutex）

​		一次只能一个线程拥有互斥锁。

​		**互斥锁是在抢锁失败的情况下主动放弃CPU进入睡眠状态直到锁的状态改变时再唤醒**，而操作系统负责线程调度，为了实现锁的状态发生改变时唤醒阻塞的线程或者进程，需要把锁交给操作系统管理，所以 互斥锁在加锁操作时涉及上下文的切换。

### （3）条件变量

​		互斥锁一个明显的缺点是他只有两种状态：锁定和非锁定。而条件变量通过允许线程阻塞和等待另一个线程发送信号的方法弥补了互斥锁的不足，他常和互斥锁一起使用，以免出现竞态条件。**当条件不满足时，线程往往解开相应的互斥锁并阻塞线程然后等待条件发生变化。一旦其他的某个线程改变了条件变量，他将通知相应的条件变量唤醒一个或多个正被此条件变量阻塞的线程。总的来说互斥锁是线程间互斥的机制，条件变量则是同步机制。**

### （4）自旋锁

​		如果进线程无法取得锁，进线程不会立刻放弃CPU时间片，而是一直循环尝试获取锁，直到获取为止。 

## 32.逻辑地址和物理地址？

​		相对地址又称逻辑地址，绝对地址又称物理地址。

​		编译时只需确定变量x存放的相对地址是100 ( 也就是说相对于进程在内存中的起始地址而言的地址)。CPU想要找到x在内存中的实际存放位置，只需要用进程的起始地址+100即可。

## 33、僵尸进程和孤儿进程？

- 僵尸进程：如果子进程先退出，父进程还没退出，那么子进程必须等到父进程捕获到了子进程的退出状态才真正结束，否则这个时候子进程就成为僵尸进程。

  **设置僵尸进程的目的是维护子进程的信息，以便父进程在以后某个时候获取。**如果一个进程终止，而该进程有子进程处于僵尸状态，那么它的所有僵尸子进程的父进程ID将被重置为1（init进程）。继承这些子进程的init进程将清理它们（也就是说init进程将wait它们，从而去除它们的僵尸状态）。

- 孤儿进程：如果父进程先退出，子进程还没退出，那么子进程的父进程将变为init进程。（注：任何一个进程都必须有父进程。 

  **一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程**。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。

## 34、Windows和Linux环境下的内存分布情况

![image-20210407172003304](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210407172003304.png)

​		通过这张图你可以看到，用户空间内存，**从低到高**分别是 7 种不同的内存段：

- 程序文件段（.text），包括二进制可执行代码； 
- 已初始化数据段（.data），包括静态常量； 
- 未初始化数据段（.bss），包括未初始化的静态变量； 
- 堆段（heap），包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（跟硬件和内核版本有关） 
- 栈段（stack），包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB 。当然系统也提供了参数，以便我们自定义大小；

## 35、一个由C/C++编译的程序占用的内存分为哪几个部分？

- 栈（stack）：地址向下增长，由编译器自动分配与释放，用以存放**函数的局部变量、函数参数、返回地址**等。其操作方式类似于数据结构中的队列，先进后出。
- 堆（heap）：地址向上增长，一般由程序员分配释放，就是malloc分配的内存块。其操作方式类似于链表（记录空闲地址空间的链表）。
- 全局区/静态区（.bss和.data段）：存放全局变量和静态变量，程序运行结束后由操作系统自动释放。在C语言中，**未初始化的放在.bss段中，初始化的放在.data中，在C++中不再区分**。
- 常量存储区（.data段）：存放常量，不允许修改，程序运行结束后自动释放
- 代码区（.text段）：存放代码，不允许修改，但可以执行。编译后的二进制文件存放在这里。

## 36.一般情况下Linux/Windows平台下栈空间的大小

- Linux环境下有操作系统决定，一般是8KB；
- Windows环境下由编译器决定，VC++6.0一般是1M。

## 37.程序从堆中动态分配内存时，虚拟内存上怎么操作？

​		页表：是一个存放在物理内存中的数据结构，记录了虚拟页和物理页的映射关系。

​		在进行动态内存分配时，例如malloc()函数或者其他高级语言中的new关键字，**操作系统会在硬盘中创建或申请一段虚拟内存空间，并更新到页表**（分配一个页表条目（PTE），使该PTE指向硬盘上这个新创建的虚拟页），通过PTE建立虚拟页和物理页的映射关系。

## 38.从堆和栈上建立对象哪个快？

- 分配和释放：堆在分配和释放时都要调用malloc或free函数，比如分配时会到堆空间去寻找足够大小的空间（多次分配后会造成内存碎片），这些都会花费一定时间，而栈不需要这些。
- 访问时间：访问堆的一个具体单元，需要两次访存：第一次取得指针，第二次获取数据，而栈只需要一次。
- 访问位置：堆的内容被操作系统交换到外存的概率比栈大，栈一般是不会被交换出去的。

## 39.常见的内存分配方式？

1. 从静态存储区域分配。内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。例如全局变量、静态变量。
2. 从栈上创建。执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时自动释放。
3. 从堆上分配，即动态内存分配。

## 40.常见内存分配错误

1. 内存分配未成功，却使用了它

   常用解决方法：使用内存前检查指针是否为NULL。

2. 内存分配成功，但未初始化就使用

3. 内存分配成功且初始化，但操作越界

4. 忘记释放内存，造成内存泄漏

5. 释放了内存却继续使用它。

   - 程序中的对象调用关系过于复杂，实在难以搞清楚某个对象究竟是否已经释放了内存，此时应该重新设计数据结构，从根本上解决对象管理的混乱局面。 
   - 函数的return语句写错了，注意不要返回指向“栈内存”的“指针”或者“引用”，因为该内存在函数体结束时被自动销毁。 
   - 使用free或delete释放了内存后，没有将指针设置为NULL，导致产生“野指针”。

# 二、C++

## 1.在main执行之前和之后执行的代码？

- main函数执行之前，主要就是初始化系统相关资源：
  - 设置栈指针；
  - 初始化静态变量和全局变量；
  - 全局对象初始化，在main之前调用构造函数；
  - 将main函数的参数argc和argv等传递给main函数，然后才运行main函数。
- main函数执行之后：
  - 全局对象的析构函数；
  - 可用atexit注册一个函数，它会在main之后执行。

## 2.堆和栈的区别？

- 申请方式：
  - 堆是用户申请，速度慢，有碎片；
  - 栈是系统分配，速度快，无碎片；
- 申请大小限制：
  - 堆向高地址拓展，内存区域不连续，大小灵活；
  - 栈向低地址拓展，内存区域连续，大小固定。
- 空间大小：
  - 堆一般是1G-4G
  - 栈默认是4M

## 3.区别以下指针类型？

```C++
int *p[10]//指针数组
int (*p)[10]//数组指针
int *p(int)//返回值是指针的函数
int (*p)(int)//函数指针
```

## 4.虚函数的实现机制

​		实现机制：虚函数通过虚函数表来实现。**虚函数的地址保存在虚函数表中，在类的对象所在的内存空间中，保存了指向虚函数表的指针（称为“虚表指针”），通过虚表指针可以找到类对应的虚函数表。**

​		虚函数表**解决了基类和派生类的继承问题和类中成员函数的覆盖问题，当用基类的指针来操作一个派生类的时候，这张虚函数表就指明了实际应该调用的函数。**

​		注：**虚函数表和类绑定，虚表指针和对象绑定**。即**类的不同的对象的虚函数表是一样的**，但是每个对象都有自己的虚表指针，来指向类的虚函数表。

## 5.虚函数表存放位置？

​		首先整理一下虚函数表的特征：

- 虚函数表是全局共享的元素，全局仅有一个，编译时就构造完成；

- 虚函数表类似一个数组，类对象中存储虚表指针（vtpr），指向虚函数表；

- 虚函数表存储虚函数的地址,即**虚函数表的元素是指向类成员函数的指针**,而类中虚函数的个数在编译时期可以确定，即虚函数表的大小可以确定,即大小是在编译时期确定的，不必动态分配内存空间存储虚函数表，所以不在堆中。

  **C++中虚函数表位于只读数据段（.rodata），也就是C++内存模型中的常量区；而虚函数则位于代码段 （.text），也就是C++内存模型中的代码区。**

## 6.虚函数的代价

- 带有虚函数的类，每一个类会产生一个虚函数表，用来存储指向徐成员函数的指针，增大类；
- 带有虚函数的类的每一个对象，都会有有一个指向虚表的指针，会增加对象的空间大小；
- 不能再是inline函数，因为inline函数在编译阶段进行替代，而虚函数表示等待，在运行阶段才能确定到低是采用哪种函数，虚函数不能是inline函数。

## 7.单继承和多继承的虚函数表结构

**编译器处理虚函数表**：

- 编译器将**虚函数表的指针放在类的实例对象的内存空间中**，该对象调用该类的虚函数时**，通过指针找到虚函数表**，**根据虚函数表中存放的虚函数的地址找到对应的虚函数。**
- 如果派生类没有重新定义基类的虚函数 A，则派生类的虚函数表中保存的是基类的虚函数 A 的地址，也就是说基类和派生类的虚函数 A 的地址是一样的。
- 如果派生类重写了基类的某个虚函数 B，则派生的虚函数表中保存的是重写后的虚函数 B 的地址，也就是说虚函数 B 有两个版本，分别存放在基类和派生类的虚函数表中。
- 如果派生类重新定义了新的虚函数 C，派生类的虚函数表保存新的虚函数 C 的地址。

### （1）无虚函数覆盖的情况：

```C++
#include <iostream>
using namespace std;

class Base
{
public:
    virtual void B_fun1() { cout << "Base::B_fun1()" << endl; }
    virtual void B_fun2() { cout << "Base::B_fun2()" << endl; }
    virtual void B_fun3() { cout << "Base::B_fun3()" << endl; }
};

class Derive : public Base
{
public:
    virtual void D_fun1() { cout << "Derive::D_fun1()" << endl; }
    virtual void D_fun2() { cout << "Derive::D_fun2()" << endl; }
    virtual void D_fun3() { cout << "Derive::D_fun3()" << endl; }
};
int main()
{
    Base *p = new Derive();
    p->B_fun1(); // Base::B_fun1()
    return 0;
}
```

![image-20210328111652562](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210328111652562.png)

![image-20210328111425396](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210328111425396.png)

​		主函数中基类的指针 `p` 指向了派生类的对象，当调用函数 `B_fun1()` 时，通过派生类的虚函数表找到该函数的地址，从而完成调用

### （2）单继承有虚函数覆盖的情况

```C++
#include <iostream>
using namespace std;

class Base
{
public:
    virtual void fun1() { cout << "Base::fun1()" << endl; }
    virtual void B_fun2() { cout << "Base::B_fun2()" << endl; }
    virtual void B_fun3() { cout << "Base::B_fun3()" << endl; }
};

class Derive : public Base
{
public:
    virtual void fun1() { cout << "Derive::fun1()" << endl; }
    virtual void D_fun2() { cout << "Derive::D_fun2()" << endl; }
    virtual void D_fun3() { cout << "Derive::D_fun3()" << endl; }
};
int main()
{
    Base *p = new Derive();
    p->fun1(); // Derive::fun1()
    return 0;
}

```

![image-20210328112333821](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210328112333821.png)

### （3）多继承无虚函数覆盖的情况

```C++
#include <iostream>
using namespace std;

class Base1
{
public:
    virtual void B1_fun1() { cout << "Base1::B1_fun1()" << endl; }
    virtual void B1_fun2() { cout << "Base1::B1_fun2()" << endl; }
    virtual void B1_fun3() { cout << "Base1::B1_fun3()" << endl; }
};
class Base2
{
public:
    virtual void B2_fun1() { cout << "Base2::B2_fun1()" << endl; }
    virtual void B2_fun2() { cout << "Base2::B2_fun2()" << endl; }
    virtual void B2_fun3() { cout << "Base2::B2_fun3()" << endl; }
};
class Base3
{
public:
    virtual void B3_fun1() { cout << "Base3::B3_fun1()" << endl; }
    virtual void B3_fun2() { cout << "Base3::B3_fun2()" << endl; }
    virtual void B3_fun3() { cout << "Base3::B3_fun3()" << endl; }
};

class Derive : public Base1, public Base2, public Base3
{
public:
    virtual void D_fun1() { cout << "Derive::D_fun1()" << endl; }
    virtual void D_fun2() { cout << "Derive::D_fun2()" << endl; }
    virtual void D_fun3() { cout << "Derive::D_fun3()" << endl; }
};

int main(){
    Base1 *p = new Derive();
    p->B1_fun1(); // Base1::B1_fun1()
    return 0;
}
```

![image-20210328112536899](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210328112536899.png)

![image-20210328112546542](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210328112546542.png)

### （4）多继承有虚函数覆盖的情况

```C++
#include <iostream>
using namespace std;

class Base1
{
public:
    virtual void fun1() { cout << "Base1::fun1()" << endl; }
    virtual void B1_fun2() { cout << "Base1::B1_fun2()" << endl; }
    virtual void B1_fun3() { cout << "Base1::B1_fun3()" << endl; }
};
class Base2
{
public:
    virtual void fun1() { cout << "Base2::fun1()" << endl; }
    virtual void B2_fun2() { cout << "Base2::B2_fun2()" << endl; }
    virtual void B2_fun3() { cout << "Base2::B2_fun3()" << endl; }
};
class Base3
{
public:
    virtual void fun1() { cout << "Base3::fun1()" << endl; }
    virtual void B3_fun2() { cout << "Base3::B3_fun2()" << endl; }
    virtual void B3_fun3() { cout << "Base3::B3_fun3()" << endl; }
};

class Derive : public Base1, public Base2, public Base3
{
public:
    virtual void fun1() { cout << "Derive::fun1()" << endl; }
    virtual void D_fun2() { cout << "Derive::D_fun2()" << endl; }
    virtual void D_fun3() { cout << "Derive::D_fun3()" << endl; }
};

int main(){
    Base1 *p1 = new Derive();
    Base2 *p2 = new Derive();
    Base3 *p3 = new Derive();
    p1->fun1(); // Derive::fun1()
    p2->fun1(); // Derive::fun1()
    p3->fun1(); // Derive::fun1()
    return 0;
}
```

![image-20210328112753546](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210328112753546.png)

![image-20210328112805644](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210328112805644.png)

## 8.new/delete和malloc/free的异同

​		相同点：

- 都可用于内存的动态申请和释放

​		不同点：

- 前者是C++关键字，后者是C/C++标准库函数；

- new自动计算要分配的空间大小，malloc需要手工计算；

- new是类型安全的，malloc不是：

  ```C++
  int *p = new float[2]; //编译错误
  int *p = (int*)malloc(2 * sizeof(double));//编译无错误
  ```

- new/delete具备构造和析构函数功能，malloc/free仅仅分配与回收空间，分配空间存储类的对象存在危险；

- new/delete返回的是具体类型指针，malloc/free返回的是void*指针，需要进行强制类型转换；

- new分配失败时会抛出bad_alloc异常，malloc分配失败时返回空指针。

- new 操作符从自由存储区上为对象动态分配内存，而 malloc函数从堆上动态分配内存（自由存储区不等于堆）。

## 9.new和delete是如何实现的？

- new首先调用operator new()函数申请空间（底层通过malloc实现），然后调用构造函数初始化，最后返回自定义类型的指针；
- delete首先调用析构函数，然后调用operator delete()释放空间（底层通过free实现）。
- malloc/free无法进行自定义类型对象的构造和析构。

## 10.malloc和free是如何实现的？

​		在标准C库中，提供了malloc/free函数分配释放内存，这两个函数底层是由brk，mmap，munmap这些系统调用实现的。

​		malloc是从堆里面申请内存，也就是说函数返回的指针是指向堆里面的一块内存。操作系统中有一个记录空闲内存地址的链表。当操作系统收到程序的申请时，就会遍历该链表，然后就寻找第一个空间大于所申请空间的堆结点，然后就将该结点从空闲结点链表中删除，并将该结点的空间分配给程序。

​		malloc的原理：

- 当开辟空间小于128K时，调用brk()函数，通过移动_enddata实现；

- 当开辟空间大于128K时，调用mmap()函数，通过在虚拟地址空间（堆和栈中间，称为文件映射区域的地方）中开辟一段内存空间实现；

- 这两种方式分配的都是虚拟内存而非物理内存**。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系；**

  malloc底层实现：

- brk() 函数实现原理：向高地址的方向移动指向数据段的高地址的指针 _enddata。

- mmap内存映射原理

  - 进程启动映射过程，并在虚拟地址空间（堆和栈中间，称为文件映射区域的地方）中为映射创建虚拟映射区域； 
  - 调用内核空间的系统调用函数 mmap()，实现文件物理地址和进程虚拟地址的一一映射关系；
  - 进程发起对这片映射空间的访问，引发缺页异常，实现文件内容到物理内存（主存）的拷贝。

## 11.malloc申请的存储空间能用delete释放吗？

​		不能.

- malloc /free主要为了兼容C，new和delete完全可以取代malloc /free的。 
- malloc/free的操作对象都是必须明确大小的，而且不能用在动态类上。
- new和delete会自动进行类型检查和大小，malloc/free不能执行构造函数与析构函数，所以动态对象它是不行的。

## 12.delete和delete[]区别？

- delete是对单个对象使用，只调用一次析构函数；
- delete[]是对数组使用，对数组中每个元素都调用一次析构函数；
- delete[]时，数组中的元素按逆序的顺序进行销毁。

## 13.delete[]如何知道释放内存的大小的？

​		需要在 new一个对象数组时，需要保存数组的维度，**C++ 的做法是在分配数组空间时多分配了 4 个字节的大小，专门保存数组的大小，在 delete [] 时就可以取出这个保存的数，就知道了需要调用析构函数多少次了。**

## 14.delete p、delete []p、allocator都有什么作用？

​		new在内存分配上面有一些局限性，new的机制是将内存分配和对象构造组合在一起，同样的，delete也是将对象析构和内存释放组合在一起的。allocator将这两部分分开进行，**allocator申请一部分内存，不进行初始化对象，只有当需要的时候才进行初始化操作。**

## 15.malloc、realloc、calloc的区别

1. malloc函数

   ```C++
   //在内存的动态存储区中分配一块长度为“size”字节的连续区域，返回该区域的首地址。
   void* malloc(unsigned int num_size);
   int *p = malloc(20*sizeof(int));//申请20个int类型的空间；
   ```

2. realloc函数

   给动态分配的空间分配额外的空间，用于扩容：

   ```C++
   void realloc(void *p, size_t new_size);
   ```

3. calloc函数

   - 省去了人为空间计算；
   - 用**malloc只分配空间不初始化**，也就是依然保留着这段内存里的数据；
   - **calloc分配的空间全部初始化为0**，这样就避免了可能的一些数据错误。

   ```C++
   //在内存的动态存储区中分配n块长度为“size”字节的连续区域，返回首地址。
   void* calloc(size_t n,size_t size);
   int *p = calloc(20, sizeof(int));
   ```

## 16.define和函数区别？

- 调用：
  - define在编译时完成替换，之后被替换的文本参与编译，运行时不存在函数调用，执行速度快；
  - 函数调用在运行时需要跳转到具体调用函数；
- 返回值：
  - define属于在结构中插入代码，无返回值；
  - 函数有返回值；
- 参数类型：
  - define参数没有类型，不进行类型检查；
  - 函数参数具有类型，需要检查。

## 17.define和typedef区别？

- 原理：
  - define是预处理指令，在编译之前进行文本替换，不做正确性检查；
  - typedef是关键字，在编译时处理，有类型检查功能。

- 功能：

  - define用于定义常量及书写复杂的内容；
  - typedef主要用于定义类型别名；

- 指针操作：typedef char * p_char和#define p_char char *区别巨大

  ```C++
  #define p_char2 char *
  typedef char * p_char1;
  p_char1 a1,b1;//相当于char *a1,*b1;
  p_char2 a2,b2;//相当于char *a1,b1;因为define只是简单的文本替换
  ```

## 18.define和inline区别？

- 原理：
  - define在编译预处理时进行简单的文本替换;
  - inline在编译时展开，内联函数直接被嵌入到目标代码中去
- 参数类型：
  - define参数没有类型，不进行类型检查；
  - inline函数是真正的函数，会对参数的类型、函数体内的语句编写是否正确等进行检查。

## 19.define和const区别？

- 编译阶段
  - define在编译预处理时进行简单的文本替换;
  - const是在编译阶段确定其值。
- 内存占用
  - define定义的宏常量，在程序中使用多少次就会进行多少次替换，内存中有多个备 份，占用的是代码段的空间；
  - const 定义的常量占用静态存储区的空间（也可以存储在栈区），程序运行过程中只有一份。

## 20.变量声明和定义的区别？

- 声明仅仅是把变量的声明的位置及类型提供给编译器，并不分配内存空间；定义要在定义的地方为其分配存储空间。
- 相同变量可以在多处声明（外部变量extern），但只能在一处定义。

## 21.strlen和sizeof区别？

- sizeof是运算符，在编译时得到结果；strlen是字符处理的库函数。
- sizeof参数可以是任何数据的类型或者数据（sizeof参数不退化）；strlen的参数只能是字符指针且结尾是'\0'的字符串。
- sizeof的值在编译时确定，所以不能用来得到动态分配存储空间的大小。

## 22.a和&a有什么区别？

```C++
int a[10];
int (*p)[10]=&a;
```

- a是数组名，表示数组首元素地址，+1表示地址值加上一个int类型的大小，如果a的值是 0x00000001，加1操作后变为0x00000005。*(a + 1) = a[1]。
- &a是数组的指针，其类型为int (*)[10]（就是前面提到的数组指针），其加1时，系统会认为是数组首地址加上整个数组的偏移（10个int型变量），值为数组a尾元素后一个元素的地址。
- 输出 *(int *)p时，其值为a[0]的值，因为被转为int *类型，解引用时按照int类型大小来读取。

## 23.野指针和悬空指针

- 野指针：没有被初始化过的指针。
- 悬空指针：指针最初指向的内存已经被释放。
- 避免野指针比较简单，但悬空指针比较麻烦。c++引入了智能指针，C++智能指针的本质就是避免悬空指针的产生。

## ？24.迭代器失效的情况

# 三、STL

## 1.STL中的hashtable的实现？

​		STL中的hashtable使用的是开链法解决hash冲突问题，如下图所示。

![image-20210407201712818](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210407201712818.png)

​		hashtable中的bucket所维护的list既不是list也不是slist，**而是其自己定义的由hashtable_node数据结构组成的linked-list**，而bucket聚合体本身使用vector进行存储。hashtable的迭代器只提供前进操作， 不提供后退操作

​		在hashtable设计bucket的数量上，其内置了28个质数[53, 97, 193,...,429496729]，在创建hashtable 时，会根据存入的元素个数选择大于等于元素个数的质数作为hashtable的容量（vector的长度），其 中每个bucket所维护的linked-list长度也等于hashtable的容量。如果插入hashtable的元素个数超过了 bucket的容量，就要进行重建table操作，即找出下一个质数，创建新的buckets vector，重新计算元素 在新hashtable的位置。

## 2.vector与list的区别与应用？

1. vector和数组类似，拥有一段连续的内存空间，并且起始地址不变。因此能高效的进行随机存取，时间复杂度为o(1);但因为内存空间是连续的，所以在进行插入和删除操作时，会造成内存块的拷贝，时间复杂度为o(n)。另外，当数组中内存空间不够时，会重新申请一块内存空间并进行内存拷贝。连续存储结构：vector是可以实现动态增长的对象数组，支持对数组高效率的访问和在数组尾端的删除和插入操作，在中间和头部删除和插入相对不易，需要挪动大量的数据。它与数组最大的区别就是vector不需程序员自己去考虑容量问题，库里面本身已经实现了容量的动态增长，而数组需要程序员手动写入扩容函数进形扩容。
2. list是由双向链表实现的，因此内存空间是不连续的。只能通过指针访问数据，所以list的随机存取非常 没有效率，时间复杂度为o(n);但由于链表的特点，能高效地进行插入和删除。非连续存储结构：list是 一个双链表结构，支持对链表的双向遍历。每个节点包括三个信息：元素本身，指向前一个元素的节点 （prev）和指向下一个元素的节点（next）。因此list可以高效率的对数据元素任意位置进行访问和插 入删除等操作。由于涉及对额外指针的维护，所以开销比较大

## 3.map、set是怎么实现的？为什么使用红黑树？

1. 他们的底层都是以红黑树的结构实现，因此插入删除等操作都在O(logn)时间内完成，因此可以完成 高效的插入删除； 
2. 在这里我们定义了一个模版参数，如果它是key那么它就是set，如果它是map，那么它就是map； 底层是红黑树，实现map的红黑树的节点数据类型是key+value，而实现set的节点数据类型是value 
3. 因为map和set要求是自动排序的，红黑树能够实现这一功能，而且时间复杂度比较低。

# 四、计算机网络

## 1.在浏览器中输入url地址后显示主页的过程？

- 根据域名，进行DNS域名解析
- 拿到解析的IP地址，建立TCP连接
- 向IP地址发送HTTP请求
- 服务器处理请求并返回响应结果
- 关闭TCP连接
- 浏览器解析HTML
- 浏览器布局渲染

## 2.DNS是什么？

​		DNS是域名系统，因特网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串

## 3.DNS工作原理？

​		DNS可以使用UDP或TCP进行传输，使用的端口号都为53，以下两种情况会使用TCP：

- 返回的响应超过512B（UDP最大只支持512B）；
- 区域传送（主域名服务器向辅助域名服务器传送变化的那部分数据）

## 4.什么是DNS负载均衡？

​		当一个网站有足够多的用户的时候，假如每次请求的资源都位于同一台机器上面，那么这台机器随时可能会崩掉。处理方法就是DNS负载均衡技术。

​		在DNS系统中有一个比较重要的的资源类型叫做主机记录也称为A记录，A记录是用于名称解析的重要记录，它将特定的主机名映射到对应主机的IP地址上。

​		在DNS服务器中应该配置了多个A记录，如：

```C++
  www.apusapp.com IN A 114.100.20.201;
  www.apusapp.com IN A 114.100.20.202;
  www.apusapp.com IN A 114.100.20.203;
```

​		它的原理是在DNS服务器中**为同一个主机名配置多个IP 地址**,每次域名解析请求都会根据对应的负载均衡算法计算出一个不同的IP地址并返回，这样A记录中配置多个服务器就可以构成一个集群，并可以实现负载均衡

![image-20210407232059939](C:\Users\Ursnus\AppData\Roaming\Typora\typora-user-images\image-20210407232059939.png)

## 5.怎么实现DNS劫持？

​		DNS 劫持即域名劫持，是**通过将原域名对应的 IP 地址进行替换从而使得用户访问到错误的网站或者使得用户无法正常访问网站的一种攻击方式。**

## 6.为什么域名解析用UDP协议？

​		UDP协议快，客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。

## 7.为什么区域传送用TCP协议？

​		TCP协议可靠性好，并且传输内容多。

## 8.HTTP长连接和短连接的区别

- 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接， 任务结束就中断连接。 

- 从HTTP/1.1起，默认使用长连接，用以保持连接特性。

## 9.TCP粘包/拆包问题？

### （1）为什么会发生TCP粘包和拆包？

- 发送方写入的数据大于套接字缓冲区的大小，此时将发生拆包；
- 进行 MSS （最大报文长度）大小的 TCP 分段，当 TCP 报文的数据部分大于 MSS 的时候将发生拆包；
-  发送方写入的数据小于套接字缓冲区大小，由于 TCP 默认使用 Nagle 算法，只有当收到一个确认后，才将分组发送给对端，当发送方收集了多个较小的分组，就会一起发送给对端，这将会发生粘包。
- 发送方发送的数据太快，接收方处理数据的速度赶不上发送端的速度，将发生粘包。

### （2）解决方法

1. **在消息的头部添加消息长度字段，**服务端获取消息头的时候解析消息长度，然后向后读取相应长度的内容。
2. 固定消息数据的长度，服务端每次读取既定长度的内容作为一条完整消息，当消息不够长时，空位补上固定字符。但是该方法会浪费网络资源。
3. 设置消息边界，也可以理解为分隔符，服务端从数据流中按消息边界分离出消息内容，一般使用换行符。

### （3）什么时候需要处理粘包问题？

​		当接收端同时收到多个分组，并且这些分组之间毫无关系时，需要处理粘包；而当多个分组属于同一数据的不同部分时，并不需要处理粘包问题。

## 10.为什么会有服务器缓存机制？如何实现的？

​		原因：

- 缓解服务器压力;
- 降低客户端获取资源的延迟：缓存通常位于内存中，读取缓存的速度更快。

​		实现方法：

1. 让代理服务器进行缓存；
2. 让客户端浏览器进行缓存。

## 11.GET和POST的区别？

1. get是获取数据，post是传输修改数据。

2. get把请求的数据放在URL上，请求参数会被完整的保留在浏览器的记录中。

   而post参数放在请求主体中，且参数不会被保留。

3. get提交的数据最大是2k（取决于浏览器），post理论上没有限制。

4. get请求只支持URL编码，post请求支持多种编码格式。

5. get只支持ASCII字符格式的参数，post没有限制。

6. get产生一个TCP数据包，浏览器会把HTTP header和data一并发送出去，服务器响应200；

   post产生两个TCP数据包，浏览器先发送header，服务器响应100 continue，浏览器再发生data，服务器响应200 ok。

7. 本质区别：get是幂等的，post不是。

> 幂等性：幂等性是指一次和多次请求某一个资源应该具有同样的副作用。简单来说意味着对同一URL的多个请求应该返回同样的结果.

​		不应该且不能用get请求做数据的增删改这些有副作用的操作。因为get请求是幂等的，**在网络不好的隧道中会尝试重试**。如果用get请求增数据，会有**重复操作**的风险，而这种重复操作可能会导致副作用.

## 12.一个TCP连接可以对应几个HTTP请求？

​		如果维持连接，一个 TCP 连接是可以发送多个 HTTP 请求的。

## 13.一个 TCP 连接中 HTTP 请求发送可以一起发送么（比如一起发三个请求，再三个响应一起接收）？

​		HTTP/1.1存在一个问题，单个TCP连接在同一时刻只能处理一个请求，意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个TCP连接里不能重叠。 		在 HTTP/1.1 存在 Pipelining 技术可以完成这个多个请求同时发送，但是由于浏览器默认关闭，所以可以认为这是不可行的。

​		在 HTTP2 中由于 Multiplexing 特点的存在，多个 HTTP 请求可以在同一个 TCP连接中并行进行。

## 14.浏览器对同一Host建立TCP连接的数量有没有限制？

​		有。Chrome最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别。

​		假设我们还处在 HTTP/1.1 时代，那个时候没有多路传输，当浏览器拿到一个有几十张图片的网页，如果图片都是HTTPS 连接并且在同一个域名下，那么浏览器在 SSL 握手之后会和服务器商量能不能用 HTTP2，如果能的话就使用 Multiplexing 功能在这个连接上进行多路传输。

​		如果发现用不了 HTTP2 呢？或者用不了 HTTPS（现实中的 HTTP2 都是在 HTTPS 上实现的，所以也就是只能使用 HTTP/1.1）。那**浏览器就会在一个 HOST上建立多个 TCP 连接，连接数量的最大限制取决于浏览器设置，这些连接会在空闲的时候被浏览器用来发送新的请求。**

## 15.HTTPS和HTTP区别？

- HTTP协议都是以明文方式发送，数据均未加密，安全性较差；

  HTTPS数据传输过程是加密的，安全性较好。

- HTTP用的是80端口，HTTPS是443端口。

- HTTPS协议需要数字认证机构申请整数。

- HTTP页面响应比HTTPS快。HTTPS除了三次握手，还需要经历一个SSL协商过程。